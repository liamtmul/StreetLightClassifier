# Classifying Street Light Fixtures Using Deep Learning

## Overview

In collaboration with Code for Charlottesville, we sought to develop an image classifier to accurately catalog the streetlights in Charlottesville. The City of Charlottesville would in turn use the classifier to take an inventory of their existing streetlight network and  better inform their planning decisions centered around pedestrian safety, neighborhood equity, environmental footprint and network expansion. Other municipalities around the state and around the country could also  use this model to classify their street light inventory. We enlisted the Google Street View Static AP to capture our streetlight image dataset. The image classifier architecture was built using a transfer learning with InceptionResNetV2 deep learning model. The generated streetlight image classifier was able to accurately classify input images at a 95% success rate among 4 distinct classes: enclosed, open, colonial and non-streetlight.  We concluded with subsequent image capture attempts coupled with updated request parameters, the developed streetlight classifier would be able to accurately  inventory a high percentage of the streetlights in the Charlottesville streetlight network. 

## Motivations 
In order for the City of Charlottesville to progress with stated initiatives to expand, modernize and maintain their existing streetlight network they must get a thorough inventory of streetlight fixture type by location. In an effort to convert from energy intensive lighting to LED bulbs it is necessary to know the type of fixture at each location. Many municipalities, such as the City of Charlottesville, have contracted their lighting out to the local utility. Changing of location of the bulbs can become rather bureaucratic, but it is relatively easy for a municipality to request the utility to change the type of bulb. 
The goal of our project is to develop a deep learning image classifier which will accurately classify streetlights by their fixture type. An accurate image classifier will provide the capacity to generate a map of each streetlight’s location in the city of Charlottesville along with its bulb compatibility. Since the bulb type is heavily dependent on the fixture class, if Charlottesville and other cities aspire to reduce there running cost and carbon footprint the transition from Metal Halide, High Pressure Sodium, and  Low Pressure Sodium bulbs to LED lights will require an accurate account of the type of fixtures that they have located across their city.  
 
Additionally, an updated catalog of the current streetlight network will allow for the complementary goals of increased pedestrian safety and improved equity across neighborhoods to move forward. Driven by these aligned goals to reduce the city’s carbon footprint, increase pedestrian safety and extend the existing network into underserved communities, a complete and thorough inventory of the existing streetlight network is of utmost importance.
 
Image classification models have well established methodologies and previously have shown significant efficacy for similar applications. Deep Learning algorithms such as Convolutional Neural Networks (CNN) have been trained on Google Street View to successfully detect and classify land use patterns \cite{Kang},  motor vehicles \cite{Timnit}, and street numbers \cite{Goodfellow}. Studies have also employed these techniques to extract features from street images to feed into other models in order to estimate house prices \cite{Law}, building ages \cite{Li}],  and street safety \cite{Naik}.
The objective of our project is to build a deep learning model to accurately identify the presence of streetlights and classify the current fixture class from images obtained from the Google Street View Static API. The development of this classifying model would greatly expedite the city's ability to generate a map of each streetlight’s location in the city of Charlottesville along with the fixture’s proper classification.  Equipped with this information the governing bodies will be better situated to address policy decisions relating to resource investment, environmental impact, equity initiatives and safety objectives. 

## Methods 
## Data Collection
Our data collection effort was basically an exercise in generating as many successful streetlight image captures of the current Charlottesville streetlight inventory. Provided with a GeoJSON file of the current Charlottesville streetlight network inventory, we obtained the metadata for each streetlight cataloged . Within this metadata we mined the recorded GPS coordinates for each streetlight and paired with the grid address, fixture type and streetlight ID of each listed streetlight. We then used the streetlight coordinates to initiate a request to the Google Street View Static API  for the metadata on the nearest panoramic image capture. Within the google image metadata were the capture coordinates for each panoramic image. Equipped with the streetlight coordinates and the image capture coordinates, we could mathematically compute the proper heading from the panoramic capture point to the streetlight. This heading value would prove to be an essential parameter when making the final image request to the Google Street View Static API for the streetlight images. Without the heading parameter we only got a street light in roughly one out of every ten images. The number of images with street lights dramatically improved when we added the bearing getting up to a 8/10 capture rate. 
In addition to the heading parameter, we also tuned the pitch parameter in the API request. Pitch determines the vertical angle of the camera relative to the capture vehicle. The default pitch is set at zero and predominantly returns a near horizontal capture. Positive values up to 90 will return images with the camera angled up and negative values down to -90 will in turn angle the camera down. Dependent on the fixture class we were attempting to capture, we adjusted the pitch parameter accordingly. For example, the Enclosed fixtures, most often associated with high traffic streets, would be positioned 4 meters up on a pole. Therefore a pitch setting of 30 would be most suitable for capturing images of enclosed streetlights. Whereas the more residential Colonial streetlight is often placed 2 meters from the ground and a pitch setting of 10 would be the more appropriate parameter setting. 
We also tuned the FOV (field of view) parameter dependent on the fixture class we were attempting to capture.  The FOV  relates to the horizontal field of view for each captured image. It is expressed in degrees from 0 to 120. By adjusting the FOV to a lower setting, the default is set at 90,  you are essentially zooming in on the image target. For those lights set back further from the road a lower FOV setting would prove to be more productive in capturing the targeted streetlight. The challenge with adjusting the FOV parameter was that it sometimes cropped out the light fixture. For example,  if we captured a colonial light  when the light FOV was set to 90 and the light isn’t in the middle of the frame, when we change the FOV to 30, the fixture would not appear in the image. With heading, pitch and FOV properly tuned, we would then initiate a Google Street View Static API request for all the images for a particular fixture class. 
The inventory of 3788 cataloged streetlights contained 14 different classes. However only three of these fixture type classes were represented in a large enough sample in the field for us to generate enough training images for the classifier. There were a total of 3293 enclosed(ENCL) fixtures, 215 colonial (COLN) fixtures and 77 open (OPEN) making up nearly 95% of all the streetlights in Charlottesville. Using the tuned parameters, separate requests were sent  to the Google Street View Static API to return images for each of these three classes. Due to the imbalance between the classes, we made subsequent requests to the API for both the COLN and OPEN with varied parameter settings. This allowed us to generate multiple images from a single streetlight in this class and assisted in mitigating the extreme imbalances between classes present in the field. 
Once all the requested images had been downloaded, we processed each one individually to examine image capture success. For those images which showed a complete streetlight we labeled the correct class name and sorted in the appropriate sub-folder for. Images which failed to capture any streetlight or partial streetlight were labeled as non-streetight and collected in a sub-folder. Images which rendered a partial streetlight were set aside. The resulting dataset consisted of 1,845 non_streetlights, 1035 enclosed, 411 colonial and 89 open for a total of 3,380 images. 

Below are the four categories:

###Colonial Lights

![colonial](https://user-images.githubusercontent.com/50001904/118058990-73bc9000-b35d-11eb-8c15-d9be9e610226.jpg)

###Enclosed
![enclosed](https://user-images.githubusercontent.com/50001904/118059312-3dcbdb80-b35e-11eb-83ce-08cd9a565467.jpg)

###Open
![open](https://user-images.githubusercontent.com/50001904/118059420-7d92c300-b35e-11eb-8949-d87bb4e49d9d.jpg)

###No Street Light in the Image
![noStreetlight](https://user-images.githubusercontent.com/50001904/118059330-4ae8ca80-b35e-11eb-985a-ddb74bae9728.jpg)
 
## Data Augmentation 

With a labeled image dataset of n < 4,000, we anticipated that validation accuracy gains could be made through regularization. Data augmentation has proven to be beneficial whether through random generation or guided practices \cite{improved}. Data warping image augmentations are usually accomplished via minor transformations, image cropping, image reflection, exposure, or altering color contrast. They have all shown success in improving model training and validation accuracy, and the expansion of the training set often leads to improved generalization for image processing models by limiting overfitting \cite{effectda}. Recent research in data augmentation has demonstrated that guided selection of the image transformations through neural nets can further improve model accuracy. Neural augmentation involves the use of a neural net to learn which data augmentations result in the most accurate classifier \cite{effectda}. An equally promising approach is to algorithmically select those transformations which result in the greatest classification loss. The dataset is then bolstered by including these transformations before training on a neural net. The resulting classifier is likely to outperform those with random augmentations and will perform just as well as other guided transformation processes \cite{ adaptiveda}.  Given the limited size of our originating dataset,  we enlisted left to right augmentation during the preprocessing stage of our model training. 



## Final Model
 
Similar to our exploratory modeling, we instituted transfer learning using the ResNet50 deep learning model for our final model architecture.  Weights were preloaded from the well known ImageNet dataset, a dataset with approximately 1000 different classes. The central tenet of transfer learning is that many of the features used to classify images in one dataset are applicable and transferable to other image classifying tasks. To adapt the base model to the streetlights classification problem, we removed the top layer from our pretrained model and substituted it with our own Softmax layer. The softmax layer assigns a probability of class assignment to each image. Our Softmax layer consists of 4 nodes since we are classifying into one of the following classes: ENCL, COLN, OPEN or non_streetlight. 
 
Prior to tuning the model, the image dataset was first subjected to a number of preprocessing steps to improve model efficiency and accuracy. All images were resized to 299 by 299  and subjected to specific input processing as required by the ResNet50 deep learning model. Similar to our baseline models, in order to increase the robustness of our smaller classes we initiated simple data augmentation by flipping random images left to right. Computational complexity was reduced and efficiency increased through rescaling the image values between 0 and 1 and organizing in batches of  32. The image datasets were uploaded to Colab,  cached and prefetched to greatly improve model runtimes. 
The tuning of the two-step model was conducted in two separate phases. In phase one, the weights were not trainable and simply imported from the “Imagenet” data set. Using the pre-loaded weights, Adam  optimization (lr=0.005) and training to minimize sparse categorical loss, the model plateaued with a .60 validation accuracy. When the outputs from this phase one model were then inputted into a fully trainable phase II model we were able to reach .89 validation accuracy using Adam optimization and much smaller learning rate (lr= 1e-5). Test set  (n=507) accuracy was recorded at .87. The tuning of optimization algorithms and associated learning rates was conducted empirically through numerous running of test models.  
Although we initially designed our transfer learning based model on the ResNet50 deep learning model, we did decide to test a number of other base models for  transfer learning  to see if increased accuracy could be achieved. We found the IncpetionResNetV2 model to provide such an improvement. Structured with similar preprocessing and tuning protocols we established earlier the use of the InceptionResNetV2 deep learning model, ultimately proved a stronger classifier, with a validation accuracy of .96 and a test accuracy (n=507) of . 93. This, our final iteration of the streetlight classifier model, has proven to be quite accurate in correctly predicting novel images.


